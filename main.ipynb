{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bebb5d3-dfd6-460f-afd8-d889ab30e19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install sentence-transformers\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03eb2c79-29ce-4bc1-92b9-f27b0945229f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import docx2txt\n",
    "import glob\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57f089c-ffad-4c46-9e26-9ebe4696dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"to_LLM\"\n",
    "output_path = \"to_LLM\\\\txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3ef367-2689-40ed-a383-747d04fa8650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory = glob.glob(input_path + '/*.doc*')\n",
    "\n",
    "for file_name in directory:\n",
    "    with open(file_name, 'rb') as infile:\n",
    "        with open(output_path + file_name[6:-5] + '.txt', 'w', encoding='utf-8') as outfile:\n",
    "            doc = docx2txt.process(infile)\n",
    "            outfile.write(doc.strip())\n",
    "\n",
    "print(\"=========\")\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45e6318-f1e7-4c33-8dce-14420c1feaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    \"\"\"\n",
    "    Encoder class for generating embeddings from textual data using a SentenceTransformer model.\n",
    "\n",
    "    Attributes: model (SentenceTransformer): The model used for generating embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = 'Alibaba-NLP/gte-multilingual-base', use_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder with the given model name and device configuration.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model to load: \"cointegrated/rubert-tiny2\"; \"Alibaba-NLP/gte-multilingual-base\"\n",
    "            use_gpu (bool): Whether to use GPU for model inference.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model name is empty.\n",
    "            RuntimeError: If the model fails to load.\n",
    "        \"\"\"\n",
    "        if not model_name.strip():\n",
    "            raise ValueError('Model name cannot be empty.')\n",
    "\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load '{model_name}'. Error: {e}\")\n",
    "\n",
    "\n",
    "    def encode(self, data: Union[List[str], str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes a list of textual data into embeddings.\n",
    "\n",
    "        Args: data (Union[List[str], str]): The list of texts or a single text to encode.\n",
    "\n",
    "        Returns: torch.Tensor: The tensor of embeddings.\n",
    "\n",
    "        Raises: RuntimeError: If encoding fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.model.encode(data, convert_to_tensor=True)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error encoding data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2052d423-5695-42ac-b7eb-b287985b701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) class for generating responses based on retrieved\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: Encoder):\n",
    "        \"\"\"\n",
    "        Initializes the RAG class with the given encoder.\n",
    "\n",
    "        Args: encoder (Encoder): The encoder to be used for encoding documents and queries.\n",
    "\n",
    "        Raises: ValueError: If the encoder is not an instance of Encoder.\n",
    "        \"\"\"\n",
    "        if not isinstance(encoder, Encoder):\n",
    "            raise ValueError(\"The encoder must be an instance of Encoder.\")\n",
    "\n",
    "        self.documents = None\n",
    "        self.doc_embeddings = None\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def splitter(self, documents_path: str):\n",
    "        splitter = CharacterTextSplitter(separator=\" \",  chunk_size=8192, chunk_overlap=1024)\n",
    "        documents = []\n",
    "        for root, directories, files in os.walk(documents_path , topdown=False):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(\".txt\"):\n",
    "                    name =(os.path.join(root,file))\n",
    "                    with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "                        file_content = f.read()\n",
    "                        file_content = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s]+', '', file_content.strip())\n",
    "                        file_content = re.sub(r'[\\n\\t]+', ' ', file_content).strip()\n",
    "                        for chunk in splitter.split_text(file_content):\n",
    "                            documents.append({'filename':name, \"chunk_content\": chunk})   #Document(page_content=chunk, metadata={'source': file}))\n",
    "        self.documents = documents\n",
    "        return documents\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Fits the RAG model and calculates embeddings for the provided documents.\n",
    "\n",
    "        Args: documents (List[str]): List of documents to be used for retrieval.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the documents list is empty.\n",
    "            RuntimeError: If there is an error encoding the documents.\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            raise ValueError(\"Documents list cannot be empty.\")\n",
    "\n",
    "        empty_documents = all(phrase['chunk_content'].strip() != '' for phrase in documents)\n",
    "        if not empty_documents:\n",
    "            raise ValueError(\"Documents cannot be empty.\")\n",
    "\n",
    "        documents = [x['chunk_content'] for x in self.documents]\n",
    "        \n",
    "        try:\n",
    "            self.doc_embeddings = self.encoder.encode(documents)\n",
    "            # self.documents = documents\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error encoding the documents: {e}\")\n",
    "\n",
    "    def retrieve(self, query: str,\n",
    "                 retrieval_limit: int = 5,\n",
    "                 similarity_threshold: float = 0.5) -> Tuple[List[int], List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieves the most relevant documents based on the query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query text.\n",
    "            retrieval_limit (int): Maximum number of documents to retrieve. Default is 5.\n",
    "            similarity_threshold (float): Threshold for document similarity\n",
    "            to be considered relevant. Default is 0.5.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[int], List[str]]: The indices of the retrieved documents\n",
    "            and the retrieved documents themselves.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the documents have not been fitted yet.\n",
    "            ValueError: If the retrieval limit is not between 1 and 10.\n",
    "            ValueError: If the retrieval limit is greater than the number of documents.\n",
    "            ValueError: If the similarity threshold is not between 0 and 1.\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            raise ValueError(\"The documents have not been fitted yet\")\n",
    "\n",
    "        if 1 > retrieval_limit > 10:\n",
    "            raise ValueError(\"The retrieval limit is not between 1 and 10\")\n",
    "\n",
    "        if retrieval_limit > self.doc_embeddings.shape[0]:\n",
    "            raise ValueError(\"The retrieval limit is greater than the number of documents\")\n",
    "\n",
    "        if 0 > similarity_threshold > 1:\n",
    "            raise ValueError(\"similarity threshold is not between 0 and 1\")\n",
    "\n",
    "        # user_embeddings = self.encoder.encode(query)\n",
    "        # cos_dist = util.pytorch_cos_sim(user_embeddings, self.doc_embeddings)\n",
    "        # cos_dist_sort = (cos_dist.argsort(descending=True)[:, :retrieval_limit]\n",
    "        #                  .reshape((-1,))\n",
    "        #                  .tolist())\n",
    "        # return cos_dist_sort, [self.documents[idx] for idx in cos_dist_sort]\n",
    "\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        similarity = util.pytorch_cos_sim(query_embedding, self.doc_embeddings)\n",
    "\n",
    "        torch_topk_values, torch_topk_indices = torch.topk(similarity, retrieval_limit)\n",
    "\n",
    "        topk_values = torch_topk_values.flatten().tolist()\n",
    "        topk_indices = torch_topk_indices.flatten().tolist()\n",
    "\n",
    "        topk_indices = [idx for score, idx in zip(topk_values, topk_indices) if score >= similarity_threshold]\n",
    "\n",
    "        retrieved_docs = [self.documents[idx] for idx in topk_indices]\n",
    "\n",
    "        return topk_indices, retrieved_docs\n",
    "\n",
    "\n",
    "    def _create_prompt_template(self, query: str, retrieved_docs) -> str:\n",
    "        \"\"\"\n",
    "        Creates a prompt template for text generation.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "            retrieved_docs (List[str]): The list of retrieved documents.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "        prompt = \"Instructions: Based on the relevant documents, generate a comprehensive response to the user's query.\\n\"\n",
    "\n",
    "        prompt += \"Relevant Documents:\\n\"\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            prompt += f\"Document {i+1}: {doc['chunk_content']}\\n\"\n",
    "\n",
    "        prompt += f\"User Query: {query}\\n\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _chadgpt_api(self, prompt):\n",
    "         # Ключ из личного кабинета, подставьте свой\n",
    "        CHAD_API_KEY = 'CHAD_API_KEY'\n",
    "\n",
    "        # Формируем запрос\n",
    "        request_json = {\n",
    "            \"message\": prompt,\n",
    "            \"api_key\": CHAD_API_KEY\n",
    "        }\n",
    "    \n",
    "        # Отправляем запрос и дожидаемся ответа\n",
    "        response = requests.post(url='https://ask.chadgpt.ru/api/public/claude-3.5-sonnet', json=request_json) \n",
    "        # https://ask.chadgpt.ru/api/public/gpt-4o-mini\n",
    "        # https://ask.chadgpt.ru/api/public/gpt-4o\n",
    "        # https://ask.chadgpt.ru/api/public/claude-3-haiku\n",
    "        # https://ask.chadgpt.ru/api/public/claude-3-opus\n",
    "        # https://ask.chadgpt.ru/api/public/claude-3.5-sonnet\n",
    "        # Проверяем, отправился ли запрос\n",
    "        if response.status_code != 200:\n",
    "            print(f'Ошибка! Код http-ответа: {response.status_code}')\n",
    "        else:\n",
    "            # Получаем текст ответа и преобразовываем в dict\n",
    "            resp_json = response.json()\n",
    "    \n",
    "            # Если успешен ответ, то выводим\n",
    "            if resp_json['is_success']:\n",
    "                return resp_json['response']\n",
    "                # used_words = resp_json['used_words_count']\n",
    "            else:\n",
    "                error = resp_json['error_message']\n",
    "                print(f'Ошибка: {error}')\n",
    "                return error\n",
    "        \n",
    "\n",
    "    \n",
    "    def _generate(self, query: str, retrieved_docs) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response based on the retrieved documents and query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "            retrieved_docs (List[str]): The list of retrieved documents.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "\n",
    "        Pseudo-code:\n",
    "            - Create a prompt using the query and retrieved documents.\n",
    "            - Pass the prompt to a text generation model.\n",
    "            - Retrieve and return the generated response.\n",
    "        \"\"\"\n",
    "        # Create the prompt template\n",
    "        prompt = self._create_prompt_template(query, retrieved_docs)\n",
    "\n",
    "        # Pass the prompt to the text generation model (example using GPT-3 or similar model)\n",
    "        # generated_response = text_generation_model.generate(prompt)\n",
    "\n",
    "        resp_msg = self._chadgpt_api(prompt)\n",
    "        \n",
    "       # Return the generated response\n",
    "        generated_response = resp_msg  # Replace with actual implementation\n",
    "\n",
    "        return generated_response\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Runs the full RAG pipeline: retrieves documents and generates a response.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        _, retrieved_docs = self.retrieve(query)\n",
    "        generated_response = self._generate(query, retrieved_docs)\n",
    "\n",
    "        return generated_response, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2253dd-2789-4e61-945e-829e6a327af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [files for _, _, files in os.walk(output_path, topdown=False)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ae445-1a65-4683-960d-5cdf3baf8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c5ee2a-52af-4030-bd9a-ad712f6461db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8ff9b0-db10-48c1-ae33-e957c3f32a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = rag.splitter(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbb6e208-3c6f-4b68-bc54-466bff427559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.fit(docs)\n",
    "# torch.save(rag.doc_embeddings, 'doc_embeddings.pt')\n",
    "emb = torch.load('doc_embeddings.pt', weights_only=True)\n",
    "rag.doc_embeddings = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95211f52-a594-4258-8c81-f30df8ca788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response, retrieved_docs = rag.run('Please provide repair instructions for dent on RH Outboard Flap of Boeing 777')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0403a51-4e5d-4b03-8731-5f69a5d3f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the relevant documents, here are the repair instructions for the dent on the right hand (RH) outboard flap of Boeing 777:\n",
      "\n",
      "1. Access the repair area on the RH outboard flap\n",
      "\n",
      "2. Verify the damage dimensions match those reported:\n",
      "- Dent size: 35mm length × 30mm width × 0.36mm depth\n",
      "- Delamination size: 57mm length × 50mm width\n",
      "- Location: Between fairings 7 and 8, 350mm from outboard flap track and 295mm from trailing edge\n",
      "\n",
      "3. Apply temporary seal using aluminum foil tape (speed tape):\n",
      "- Follow SRM 57-53-01 requirements\n",
      "- Ensure speed tape overlaps damaged area by minimum 25mm\n",
      "\n",
      "4. Inspection Requirements:\n",
      "- Every 10 flight cycles perform:\n",
      "  * Visual inspection\n",
      "  * Tap test inspection per NDT Manual Part 1 51-05-01 to check for damage growth\n",
      "- Reapply aluminum foil tape after inspections\n",
      "\n",
      "Important Notes:\n",
      "1. This is a temporary repair valid until April 14, 2024 or next A-check, whichever comes first\n",
      "2. At the end of repair work:\n",
      "   - Ensure no tools, debris or other items are left in repair area\n",
      "   - Return aircraft to serviceable condition\n",
      "\n",
      "If any deviations or damage growth is found during inspections, further instructions must be requested before next flight to return aircraft to serviceable condition.\n",
      "\n",
      "The repair must either be replaced or have its technical justification reviewed no later than April 14, 2024 or at the next A-check maintenance, whichever occurs first.\n"
     ]
    }
   ],
   "source": [
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90026117-2517-4084-ba2b-35b97b36e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x['filename'] for x in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ca2e3-337a-4a4d-a268-a653fe8d8b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
